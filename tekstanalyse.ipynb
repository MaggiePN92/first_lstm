{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tekstanalyse.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_9IPBzX2Lb_"
      },
      "source": [
        "# LSTM: Predikere virkeligheten av katastrofetweets\n",
        "\n",
        "Ressurser:\n",
        "- https://github.com/bentrevett/pytorch-sentiment-analysis\n",
        "- https://github.com/thanhtut/info284_lab/tree/master/lab8\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmfMb3mTUVvF"
      },
      "source": [
        "Konklusjon:\n",
        "- Vi oppnår nøyaktighet rundt 80% til tross for lite og rotete data.\n",
        "<br><br>\n",
        "\n",
        "Neste gang: \n",
        "- inkluder kilder nederst i celle.\n",
        "- lage program for justering av hyperparamtere.\n",
        "- loggføre endringer i hyperparametere og nøyaktighet tilknyttet ulike hyperparametere."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atb03vteI-Sk"
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CTD0k0mI6eg"
      },
      "source": [
        "#MOUNT DRIVE\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#CONTENT_DRIVE = './content/drive'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OCBuMoCEnhu"
      },
      "source": [
        "<h1> Datahåndtering </h1> \n",
        "<p> I denne tekstboksen presenteres hva vi gjorde med tekstdataen. </p>\n",
        "<br>\n",
        "<h2> Hva: </h2>\n",
        "<ul> \n",
        "<li>Endre smilies til placeholders</li>\n",
        "<li>Fra caps til småbokstaver, merkes med &ltcaps&gt </li>\n",
        "<li>Hashtags endres til &lthashtag&gt </li>\n",
        "<li>Forlengede og gjentatte ord endres og blir tildelt placeholders</li>\n",
        "<li>Tall, brukernavn og URLer får placeholder </li>\n",
        "<li>Mellomrom mellom !, ?, - eller = og tekst </li>\n",
        "</ul> \n",
        "<br>\n",
        "<p>Notat: \n",
        "<br> - vi har problemer med encoding. Vi valgte å ikke fokusere på dette da vi er usikre på om det vil være fruktbart for fremtidige analyser.<br> - datahåndteringen vår hadde liten effekt på nøyaktighet. Vi tror dette kan skyldes mye uren data og for lite data.</p>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "Se: \n",
        "- https://docs.python.org/3/library/re.html (regex API)\n",
        "- https://gist.github.com/tokestermw/cb87a97113da12acb388 (datahåndtering, upolert)\n",
        "- https://gist.github.com/elyase/839bf4354e442a691d04e48d0df402dd (datahåndtering)\n",
        "- https://www.freeformatter.com/html-escape.html (Problemer med formateringer?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qldmN1ErMAy"
      },
      "source": [
        "PATH = \"/content/drive/My Drive/data-tekstanalyse/\"\n",
        "train = 'df_train_split.csv'\n",
        "test = 'df_test_split.csv'\n",
        "\n",
        "df = pd.read_csv(PATH+train)\n",
        "\n",
        "import regex as re\n",
        "FLAGS = re.MULTILINE | re.DOTALL\n",
        "\n",
        "def hashtag(text):\n",
        "  text = text.group()\n",
        "  hashtag_body = text[1:]\n",
        "  if hashtag_body.isupper():\n",
        "      result = \"<hashtag> {} <allcaps\".format(hashtag_body.lower())\n",
        "  else:\n",
        "      result = \" \".join([\"<hashtag>\"] + re.split(r\"(?=[A-Z])\", hashtag_body, flags=FLAGS))\n",
        "  return result\n",
        "\n",
        "\n",
        "def allcaps(text):\n",
        "    text = text.group()\n",
        "    return text.lower() + \" <allcaps> \"\n",
        " \n",
        "#def format_fixer(text):\n",
        "\"\"\" is replaced with &quot;\n",
        "& is replaced with &amp;\n",
        "< is replaced with &lt;\n",
        "> is replaced with &gt;\"\"\"\"\n",
        "#\n",
        "#  def re_sub(pattern, repl):\n",
        "#    return re.sub(pattern, repl, text, flags=FLAGS)\n",
        "#\n",
        "#  text = re_sub(r\"&amp;\",r\"and\")\n",
        "#  text = re_sub(r\"&quot;\",r\"\\\"\")\n",
        "#  text = re_sub(r\"&lt;\",r\"<\")\n",
        "#  text = re_sub(r\"&gt;\",r\">\")\n",
        "#  return text\n",
        "\n",
        "def tokenize(text):\n",
        "    # Different regex parts for smiley faces\n",
        "    eyes = r\"[8:=;]\"\n",
        "    nose = r\"['`\\-]?\"\n",
        "\n",
        "    # function so code less repetitive\n",
        "    def re_sub(pattern, repl):\n",
        "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
        "\n",
        "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n",
        "    text = re_sub(r\"@\\w+\", \"<user>\")\n",
        "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n",
        "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n",
        "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n",
        "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n",
        "    text = re_sub(r\"/\",\" / \") #Hva gjør denne?\n",
        "    text = re_sub(r\"<3\",\"<heart>\")\n",
        "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n",
        "    text = re_sub(r\"#\\S+\", hashtag)\n",
        "    text = re_sub(r\"([!?.=-]){2,}\", r\"\\1 <repeat>\")\n",
        "    text = re_sub(r\"([!?.]){1,}\", r\" \\1 \")\n",
        "    #text = re_sub(r\"[?]\",r\" ? \")\n",
        "    #text = re_sub(r\"[!]\",\" ! \")\n",
        "    #text = re_sub(r\"ûª\",\"'\")\n",
        "    #text = re_sub(r\"ûï\",\"\")\n",
        "    #text = re_sub(r\"û_\",\"\")\n",
        "    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n",
        "\n",
        "    ## -- I just don't understand why the Ruby script adds <allcaps> to everything so I limited the selection.\n",
        "  \n",
        "    #text = re_sub(r\"([^a-z0-9()<>'`\\-]){2,}\", allcaps)\n",
        "    text = re_sub(r\"([A-Z]){2,}\", allcaps)\n",
        "    #text = format_fixer(text)\n",
        "    return text.lower()\n",
        "    \n",
        "\n",
        "#import unicodedata\n",
        "\n",
        "\n",
        "def clean2(dataframe):\n",
        "  clean_list = []\n",
        "  for index, row in df.iterrows():\n",
        "    #try:\n",
        "     # clean_list.append((tokenize(row.text).lower(),row.target))\n",
        "    #except:\n",
        "     # clean_list.append((row.text.lower(),row.target))\n",
        "     #row.text = row.text.decode('utf-8')\n",
        "     #row.text = unicodedata.normalize('NFKD', row.text).encode('ascii','ignore')\n",
        "     #row.text.decode(\"utf-8\").encode(\"windows-1252\").decode(\"utf-8\")\n",
        "\n",
        "     clean_list.append((tokenize(row.text),row.target))\n",
        "  return clean_list\n",
        "\n",
        "    \n",
        "def clean_frame(dataframe, set_test_size = 0.1):\n",
        "    clean_list = clean2(dataframe)\n",
        "    clean_dataset = pd.DataFrame(clean_list, columns = [\"text\",\"target\"])\n",
        "    \n",
        "    #split dataset into training and testing, test is 10% of dataset\n",
        "    #data_train, data_test = train_test_split(clean_dataset, test_size = set_test_size)\n",
        "    #return data_train, data_test\n",
        "    return clean_dataset\n",
        "\n",
        "data_re = clean_frame(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxpnM-pDhd4e",
        "outputId": "c2b95382-de64-4596-e3a5-2092e9dc8c74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "data_re"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>even then our words slip and souls coincide fi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>had a minute alone with my crush    .     &lt;rep...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>look:  i  have collapsed &lt;hashtag&gt; after attem...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;url&gt; suicide bomber kills &lt;number&gt; in saudi s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>you call them weekends    .     i call them bl...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260</th>\n",
              "      <td>i'm my own woman crush     ?     &lt;repeat&gt;</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>261</th>\n",
              "      <td>i keep it out down drown their insults out wit...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262</th>\n",
              "      <td>we just happened to get on the same road right...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263</th>\n",
              "      <td>young children among those rescued from capsiz...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>264</th>\n",
              "      <td>&lt;hashtag&gt; scs &lt;allcaps&gt; eestapreparando light ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>265 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text  target\n",
              "0    even then our words slip and souls coincide fi...       0\n",
              "1    had a minute alone with my crush    .     <rep...       0\n",
              "2    look:  i  have collapsed <hashtag> after attem...       0\n",
              "3    <url> suicide bomber kills <number> in saudi s...       1\n",
              "4    you call them weekends    .     i call them bl...       0\n",
              "..                                                 ...     ...\n",
              "260          i'm my own woman crush     ?     <repeat>       0\n",
              "261  i keep it out down drown their insults out wit...       0\n",
              "262  we just happened to get on the same road right...       0\n",
              "263  young children among those rescued from capsiz...       1\n",
              "264  <hashtag> scs <allcaps> eestapreparando light ...       1\n",
              "\n",
              "[265 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYNQGiRDrWla"
      },
      "source": [
        "#Read original data\n",
        "#PATH_train = \"/content/drive/My Drive/data-tekstanalyse/train.csv\"\n",
        "#PATH_test = \"/content/drive/My Drive/data-tekstanalyse/test.csv\"\n",
        "\n",
        "#df_train = pd.read_csv(PATH_train)\n",
        "#df_test = pd.read_csv(PATH_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw7B1OWKriMr"
      },
      "source": [
        "#make two csv-files\n",
        "df_train_split, df_test_split = train_test_split(data_re, test_size = 0.2, random_state = 0)\n",
        "\n",
        "\n",
        "df_test_split.to_csv('/content/drive/My Drive/data-tekstanalyse/df_test_split.csv',index = False)\n",
        "df_train_split.to_csv('/content/drive/My Drive/data-tekstanalyse/df_train_split.csv', index = False)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHOR7LH4ralt"
      },
      "source": [
        "##MAKE ITERATOR FOR CLEAN DATA\n",
        "\n",
        "#Make fields\n",
        "#TEXT = data.Field(tokenize = 'spacy', include_lengths = True)\n",
        "\n",
        "TEXT = data.Field(include_lengths = True)\n",
        "LABEL = data.LabelField(dtype = torch.float)\n",
        "\n",
        "PATH = \"/content/drive/My Drive/data-tekstanalyse/\"\n",
        "fields = [('text', TEXT),('target', LABEL)]\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/data-tekstanalyse/df_test_split.csv')\n",
        "\n",
        "#Load data\n",
        "train_data, test_data = data.TabularDataset.splits(\n",
        "                                        path = PATH,\n",
        "                                        train = 'df_train_split.csv',\n",
        "                                        test = 'df_test_split.csv',\n",
        "                                        format = 'csv',\n",
        "                                        fields = fields,\n",
        "                                        skip_header = True\n",
        ")\n",
        "\n",
        "#Split data\n",
        "SEED = 1234\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED), split_ratio=0.80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH7VjyQwhUjE",
        "outputId": "53b2d6b2-5f2d-4c86-b2c7-188d986929b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "print(vars(train_data.examples[0]))\n",
        "print(vars(valid_data.examples[0]))\n",
        "print(vars(test_data.examples[0]))\n",
        "\n",
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['<hashtag>', 'worldnews', 'fears', 'over', 'missing', 'migrants', 'in', 'med', '-', 'bbc', '<allcaps>', 'news', '-', 'home:', 'rescuers', 'search', 'for', 'survivors', 'after', 'a', 'boat', 'carr', '.', '<repeat>', '<url>'], 'target': '1'}\n",
            "{'text': ['petition', '|', 'heartless', 'owner', 'that', 'whipped', 'horse', 'until', 'it', 'collapsed', 'is', 'told', 'he', 'can', 'keep', '<allcaps>', 'his', 'animal', '!', 'act', 'now', '!', '<url>'], 'target': '0'}\n",
            "{'text': ['achievement', 'unlocke<smile>', 'replaced', 'light', 'socket;', 'did', 'not', 'electrocute', 'self'], 'target': '0'}\n",
            "Number of training examples: 170\n",
            "Number of validation examples: 42\n",
            "Number of testing examples: 53\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpeSUFbYcM_z"
      },
      "source": [
        "<h1> Hyperparametere </h1>\n",
        "\n",
        "<ul> \n",
        "  <li><strong>MAX_VOCAB_SIZE:</strong> Hvor mange ord skal nettverket lære seg, merker ukjente ord med &lt;unk&gt;. </li>\n",
        "  <li><strong>BATCH_SIZE:</strong> Hvor mange treningseksempler benyttes i hver iterasjon. </li>\n",
        "  <br>\n",
        "\n",
        "  <li><strong>INPUT_DIM</strong> = len(TEXT.vocab): Størrelse på input dimensjon (hvor mange noder). </li>\n",
        "  <li><strong>EMBEDDING_DIM:</strong> Dimensjon på embedding layer. </li>\n",
        "  <li><strong>OUTPUT_DIM:</strong> Dimensjon på output. </li>\n",
        "  <br>\n",
        "\n",
        "  <li><strong>PAD_IDX</strong> = TEXT.vocab.stoi[TEXT.pad_token]; dette er hvordan padding blir merket: &lt; pad &gt; </li>\n",
        "  <li><strong>UNK_IDX:</strong> Hvordan unk blir merket: &lt;unk&gt;  </li>\n",
        "  <li><strong>pretrained_embeddings:</strong> embeddings hentet gjennom glove </li>\n",
        "  <br>\n",
        "\n",
        "  <li><strong>N_LAYERS:</strong> Antall lag i LSTM. </li>\n",
        "  <li><strong>BIDIRECTIONAL:</strong> Se lengre ned i dokumentet. </li>\n",
        "  <li><strong>DROPOUT:</strong> Kan anses som grad av regualisering (motsatte av overfitting; når modellen blir for tilpasset treningsdataen slik at det går utover nøyaktigheten på testdataen). Høyere dropout rate fører til mer regualisering. Regualiseringen gjøres gjennom å forstyrre tilfeldige nøyroner i nettverket. </li>\n",
        "  \n",
        "  <br>\n",
        "  <li><strong>optimizer:</strong> Hvilken optimeringsalgoritme som skal benyttes. Nøyrale nettverk har som mål redusere antall feilklassifiseringer. Dette gjøres ved å beregne gradienter. Gradientene blir beregnet med ulike algoritmer. De vanligste algoritmene er Adam og SDG. Se https://pytorch.org/docs/stable/optim.html for tilgjengelig alogritmer. </li>\n",
        "  <li><strong>Learning rate (lr):</strong> Denne parameteren angir hvor lange \"steg\" optimeringsalgoritmen skal ta i hver iterasjon. Lange steg gir en raskere læringsprosess men kan føre til at en bommer på \"målet\" (målet vil være et globalt minimum for kostnadsfunksjonen). Korte steg gir en tregere læringsprosess, men kan føre til at en kommer nærmere globalet minimum. </li>\n",
        "  <li><strong>criterion:</strong> angir hvordan loss beregnes, det kan anses som et mål på avstand mellom predikert svar og riktig svar. Vi bruker BCEWithLogitLoss, se https://pytorch.org/docs/master/generated/torch.nn.BCEWithLogitsLoss.html og https://medium.com/dair-ai/building-rnns-is-fun-with-pytorch-and-google-colab-3903ea9a3a79. </li>\n",
        "  <li><strong>nr of epochs:</strong> Antall iterasjoner som brukes i læringsprosessen.</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYJqm-lLWNYR"
      },
      "source": [
        "**Max vocab size**\n",
        "\n",
        "\n",
        "\n",
        "*   MAX_VOCAB_SIZE setter vi et tak på hvor mange unike ord (eller tegn) glove skal ta med i maskinlæringa. Ordene blir rangert etter hvor mange ganger det forekommer. I dette tilfellet vil de 10.000 mest brukte ordene bli tatt med. Hvor mange ord man bør ta med vil avhenge av hvor mange unike ord som finnes i datasettet. \n",
        "*   Det er viktig å merke seg at unike ord blir identifisert i vid forstand. F.eks. regnes '?' og '**' som unike ord. Dette gjelder også varianter av samme ord slik som 'er' og 'var', 'ku' og 'kyr' eller 'ski' og 'skiene'. (Merk at glove er trent på engelske ord). Samtidig vil også 'hei!' og 'hei' kunne gjennkjennes som egne ord med mindre man har tatt høyde for dette i data rensinga.\n",
        "* Avhengig av hva slags data man har kan det være tilfeller der man ønsker å beholde slike variasjoner eller ensrette slike variasjoner. Dette gjøres i data rensingen.\n",
        "* Det antallet vi har her med 10_000 er ganske stort med tanke på dataene vi bruker. Dette vil ha stor innvirkningen på treffsikkerheten i nettverket da man tar med lite brukte ord, og det kan ofte være lurt å fokusere på et mindre antall unike ord.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w1WjX7fkhvx"
      },
      "source": [
        "<strong>Embeddings </strong> <br>\n",
        "<ul>\n",
        "  <li>RNN med LSTM har et lag kalt embedding layer. Dette laget ligger mellom input-laget (hvor ord blir sendt inn) og LSTM-laget. Input-laget mottar informasjon som one-hot vektorer, og sender dem videre til embedding-laget. \n",
        "  </li>\n",
        " <li>La oss si at du har laget en ordbok med 4 ord: \"jeg\", \"liker\", \"is\", \"elsker\". Med one-hot vektorer vil \"jeg\" og \"liker\" være like nærme hverandre som \"liker\" og \"elsker\". For mennekser er det lett å se at \"liker\" og \"elsker\" har semantiske likhetstrekk. I embedding-laget er ikke dette lengre tilfellet. Ord med lik sematisk betydning nærme hverandre i tensoren (embedding-laget blir representert i en fledimensjonal vektor (tensor)). (https://www.quora.com/What-is-the-embedding-layer-in-LSTM-long-short-term-memory)\n",
        " </li>\n",
        " <li>Vi benytter oss av en forhåndslært tensor som er konstruert av alogritmen glove (https://nlp.stanford.edu/projects/glove/). Denne er trent på engelske ord. </li>\n",
        " <li>Ved hjelp av glove unngår vi å starte fra \"scratch\". Typisk vil et nøyralt nettverk ha et tilfeldig utgangspunkt. Med glove tensoren slipper vi dette, og læringsprosessen er mye raskere. </li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMhl12pZl3Zg"
      },
      "source": [
        "#VOCAB FOR DIRTY DATA\n",
        "MAX_VOCAB_SIZE = 10_000\n",
        "\n",
        "TEXT.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_) #initialiserer <unk> i embedding til tall hentet fra normalfordeling\n",
        "\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW3HH4cGh8eb"
      },
      "source": [
        "#ITERATOR FOR CLEAN DATA\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key = lambda x: len(x.text),\n",
        "    sort_within_batch = True,\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bykpYZrfd9RN"
      },
      "source": [
        "<h1>Layout of network</h1>\n",
        "<h2>Bidirectional long short-term memory feedback network for tekstanalyse.</h2>\n",
        "\n",
        "<strong>LSTM (long short-term memory)</strong> er en måte å behandle inputene i en RNN på. Den vil i de fleste tilfeller være bedre å bruke og er samtidig beheftet med færre problemer, slik som vanishing gradient, samenlignet med standard RNN. For å forstå hva LSTM gjør er det viktig å huske på hva en RNN gjør. RNN går kort fortalt ut på at modellen leser inn mindre stykker av f.eks. en lengre tekst og samtidig bevarer det den har lest før. Med andre ord behandler modellen en ny input som en funksjon av gjeldende input pluss en 'state' som representerer output fra foregående steg. Dette er en forenkling av hvordan vi mennesker prosesserer ny informasjon, f.eks. leser en tekst: Vi leser ordene i hvert øyekast (input) og vurderer dette opp mot de foregående ordene (state). Det har vist seg at selv om state teoretisk skal kunne ta med seg informasjon fra 'langt tilbake' og lære av dette, er det vanskelig i praksis, blant annet fordi relativt små signaler fra langt tilbake har en tendens til å viskes ut. LSTM løser dette ved å legge til en egen prosess 'carry track', som tar med seg informasjon fra foregående steg på i en separat prosess. State er i LSTM et resultat av både foregående output, men også rå inputen fra forrige steg. Samtidig er behandlingen av hver ny input i LSTM RNN et resultat av ikke bare nåværende input og state (som i standard RNN), men også carry fra alle foregående steg.\n",
        "\n",
        "<strong>Bidirectional</strong> innebærer at nodene i et lag av nettverket kan bruke informasjon fra tidligere òg framtidige input i modellen, altså trekke på nabonodene i samme 'linje'. Dette er særlig nyttig i tilfeller der konteksten er viktig. Et eksempel er i oversettelse der betydningen av et ord kan kjenngjennes bedre hvis modellen får se på ordet før og etter et ord. Bidirectional er en type RNN (recurrent neural network), og sammen med LSTM er særlig egnet til å arbeide med tekst og språk, f.eks. tale-til-tekst og kjengjenning av håndskrift.\n",
        "\n",
        "Vi prøvde å ha to LSTM etter hverandre. Det forbedredet ikke nøyaktigheten til modellen. \n",
        "\n",
        "Se:\n",
        "- https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/\n",
        "- https://en.wikipedia.org/wiki/Long_short-term_memory\n",
        "- https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "- https://arxiv.org/pdf/1801.01078.pdf (om RNN dens utvikling)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkrbJsRxJ-4R"
      },
      "source": [
        "#lay-out of RNN\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout, pad_idx, n_layers2 = None, dropout2 = None):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        #embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        #LSTM-layers\n",
        "        self.rnn = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout)\n",
        "        \n",
        "        #self.rnn2 = nn.LSTM(hidden_dim, \n",
        "        #                   hidden_dim, \n",
        "        #                   num_layers=n_layers2,\n",
        "        #                    bidirectional=bidirectional, \n",
        "        #                   dropout=dropout2)\n",
        "        \n",
        "        #fully connected\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        #pack sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        \n",
        "        #unpack sequence\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * num directions]\n",
        "        #output over padding tokens are zero tensors\n",
        "        \n",
        "        #hidden = [num layers * num directions, batch size, hid dim]\n",
        "        #cell = [num layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "        #and apply dropout\n",
        "        \n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "                \n",
        "        #hidden = [batch size, hid dim * num directions]\n",
        "            \n",
        "        return self.fc(hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_Xo6CvB0IRq",
        "outputId": "0284427e-1007-4d99-c5cd-4b0b2ea8b1c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "#Hyperparameters for clean data\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 5\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.4\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "#N_LAYERS2 = 3\n",
        "#DROPOUT_2 = 0.2\n",
        "\n",
        "\n",
        "#pretrained embeddings\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "#index for unkown words\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "#model with hyperparam\n",
        "model = RNN(INPUT_DIM, \n",
        "            EMBEDDING_DIM, \n",
        "            HIDDEN_DIM, \n",
        "            OUTPUT_DIM, \n",
        "            N_LAYERS, \n",
        "            BIDIRECTIONAL, \n",
        "            DROPOUT, \n",
        "            PAD_IDX,\n",
        "            )\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "print(model.embedding.weight.data)\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.8438,  1.4076, -0.5572,  ...,  0.3167,  0.0515,  0.6809],\n",
            "        ...,\n",
            "        [-1.0433,  0.1767, -1.1038,  ...,  0.3532,  0.7137,  0.2933],\n",
            "        [ 0.7033, -0.1868,  2.2958,  ...,  2.3021, -0.5277,  2.0576],\n",
            "        [ 0.2183,  1.0443, -1.1184,  ..., -0.3706,  0.3375,  0.4192]])\n",
            "torch.Size([1252, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUCtDPSc1kzv",
        "outputId": "4a8661e2-799e-49b2-af46-f6547fe97168",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 7,166,737 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZcRy6AhrCQi",
        "outputId": "db2c7ac2-de28-46d4-fe43-fb8dab04d78d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.1744, -0.2356, -0.6259,  ..., -0.6309,  0.1220, -0.5204],\n",
              "        [ 1.9446, -0.4448, -1.2926,  ...,  0.9853,  0.2777,  1.1442],\n",
              "        [-0.3398,  0.2094,  0.4635,  ..., -0.2339,  0.4730, -0.0288],\n",
              "        ...,\n",
              "        [ 1.2011,  0.5408,  0.2319,  ...,  1.0922, -0.1467,  0.8313],\n",
              "        [ 1.8574,  0.1586, -0.9055,  ..., -0.8275,  0.8096,  0.1784],\n",
              "        [-1.1062,  0.3486,  0.9329,  ..., -1.5898, -1.1218, -0.4589]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HeaY0JRrCa-",
        "outputId": "31144b06-6d9d-49b7-cca8-5661ddf3bd3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.3398,  0.2094,  0.4635,  ..., -0.2339,  0.4730, -0.0288],\n",
            "        ...,\n",
            "        [ 1.2011,  0.5408,  0.2319,  ...,  1.0922, -0.1467,  0.8313],\n",
            "        [ 1.8574,  0.1586, -0.9055,  ..., -0.8275,  0.8096,  0.1784],\n",
            "        [-1.1062,  0.3486,  0.9329,  ..., -1.5898, -1.1218, -0.4589]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "588ytvibrCjy"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.0009)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAiPIYu5rKhs"
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAIFgiO2rKlo"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        text, text_lengths = batch.text\n",
        "        \n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.target)\n",
        "        \n",
        "        acc = binary_accuracy(predictions, batch.target)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3gqrnyirKph"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            text, text_lengths = batch.text\n",
        "            \n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.target)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.target)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khbHIC_GrQ2f"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKvfTM7YrQ86",
        "outputId": "803cdd7e-405b-44c1-be05-d5ee7e063c44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "N_EPOCHS = 6\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.692 | Train Acc: 51.79%\n",
            "\t Val. Loss: 0.698 |  Val. Acc: 52.38%\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.675 | Train Acc: 56.03%\n",
            "\t Val. Loss: 0.698 |  Val. Acc: 52.38%\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.623 | Train Acc: 58.41%\n",
            "\t Val. Loss: 0.671 |  Val. Acc: 52.38%\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.629 | Train Acc: 67.58%\n",
            "\t Val. Loss: 0.650 |  Val. Acc: 57.14%\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.621 | Train Acc: 72.74%\n",
            "\t Val. Loss: 0.607 |  Val. Acc: 78.57%\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.518 | Train Acc: 76.17%\n",
            "\t Val. Loss: 0.598 |  Val. Acc: 71.43%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO66Q6vurQ6D",
        "outputId": "2818b20d-39ba-47dd-fdf4-7deb75c73207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#model.load_state_dict(torch.load('tekstanalyse.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.607 | Test Acc: 64.15%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ui8o0XxJQxJT"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def predict_sentiment(model, sentence):\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
        "    return prediction.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrS73U-HQ0-j",
        "outputId": "92c1e473-73ac-4670-9240-bd0c184cb826",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        }
      },
      "source": [
        "i = 0\n",
        "for text in data_re['text']:\n",
        "  predicted_label = 0\n",
        "  pred = predict_sentiment(model,text)\n",
        "  if pred < 0.5: predicted_label = 0\n",
        "  else: predicted_label = 1\n",
        "  i += 1 \n",
        "  print(f'Sentence: {str(text)} | Prediction score: {round(pred,2)} | Predicted label: {predicted_label}')\n",
        "\n",
        "  if i == 20: break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: even then our words slip and souls coincide finer than subatomic spells just as we collide <url> | Prediction score: 0.21 | Predicted label: 0\n",
            "Sentence: had a minute alone with my crush    .     <repeat>it was an overrated experience    .     <repeat>smh | Prediction score: 0.41 | Predicted label: 0\n",
            "Sentence: look:  i  have collapsed <hashtag> after attempting to munch an endangered species    .     | Prediction score: 0.47 | Predicted label: 0\n",
            "Sentence: <url> suicide bomber kills <number> in saudi security site mosque - reuters <url> | Prediction score: 0.88 | Predicted label: 1\n",
            "Sentence: you call them weekends    .     i call them bloody mary times    .     this summer's been full of them    .     my new <url> | Prediction score: 0.27 | Predicted label: 0\n",
            "Sentence: san antonio stars head coach dan hughes was just carted to the locker room after one of his guards collided with    .     <repeat> <url> | Prediction score: 0.74 | Predicted label: 1\n",
            "Sentence: big data and social information explosion: the union that could evolve your retail strategy    .     <repeat>hu <allcaps> a | Prediction score: 0.59 | Predicted label: 1\n",
            "Sentence: today japan marks <number> yrs since the u    .    s (a) bombed <number> cities killing over <number> people but we have to worry about iran <url> | Prediction score: 0.58 | Predicted label: 1\n",
            "Sentence: <hashtag> shipping <hashtag> logistics enca <allcaps>  | fatalities as migrant boat capsizes in med with hundreds onboar<smile>  <elong>capsized as i    .     <repeat>  <url> | Prediction score: 0.63 | Predicted label: 1\n",
            "Sentence: and so it begins    .     <repeat> day one of the snow apocalypse | Prediction score: 0.67 | Predicted label: 1\n",
            "Sentence: obama declares disaster for typhoon-devastated saipan: obama signs disaster declaration for northern marians a    .     <repeat> <url> | Prediction score: 0.77 | Predicted label: 1\n",
            "Sentence: think i'll become a businessman a demolish a community centre and build condos on it but foiled by a troupe of multi-racial breakdancers     .     | Prediction score: 0.4 | Predicted label: 0\n",
            "Sentence: <hashtag> hot  funtenna: hijacking computers to send data as sound waves [black hat <number>] <url> <hashtag> prebreak <hashtag> best | Prediction score: 0.57 | Predicted label: 1\n",
            "Sentence: just stop fucking saying a whole û÷notherû    .     it just sounds fucking stupid    .     you fucking mean a whole otherû    .     not a fucking tongue-twister    .     | Prediction score: 0.2 | Predicted label: 0\n",
            "Sentence: truth    .     <repeat>\n",
            "<url>\n",
            "<hashtag> news\n",
            " bbc \n",
            " cnn \n",
            "<hashtag> islam\n",
            "<hashtag> truth\n",
            "<hashtag> god\n",
            " isis \n",
            "<hashtag> terrorism\n",
            "<hashtag> quran\n",
            "<hashtag> lies <url> | Prediction score: 0.62 | Predicted label: 1\n",
            "Sentence: japan marks <number>th anniversary of hiroshima atomic bombing <url> | Prediction score: 0.82 | Predicted label: 1\n",
            "Sentence: suicide bomber kills <number> in saudi security site mosque - a suicide bomber killed at least <number> people in an attack on    .     <repeat> <url> | Prediction score: 0.88 | Predicted label: 1\n",
            "Sentence: [high <allcaps>  priority <allcaps> ] severe <allcaps>  thunderstorm <allcaps>  watch <allcaps>  ended <allcaps>  issued for lethbridge [update<smile> aug <number>th <number> mdt <allcaps> ] <url> | Prediction score: 0.65 | Predicted label: 1\n",
            "Sentence: man found dead in demi moore's swimming pool    !     <url> | Prediction score: 0.65 | Predicted label: 1\n",
            "Sentence: aftershock was the most terrifying best roller coaster i've ever been on    .     *disclaimer <allcaps> * i've been on very few    .     | Prediction score: 0.38 | Predicted label: 0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}